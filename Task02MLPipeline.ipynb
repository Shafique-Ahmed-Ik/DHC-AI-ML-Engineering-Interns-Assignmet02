{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd59aef7-cb7c-4327-8288-b80f96cce85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# Task 2: End-to-End ML Pipeline with Scikit-learn Pipeline API\n",
    "# Objective:\n",
    "#   Build a reusable ML pipeline to predict customer churn\n",
    "#   using the Telco Churn dataset.\n",
    "# Skills:\n",
    "#   - Preprocessing (scaling, encoding)\n",
    "#   - Building Scikit-learn Pipelines\n",
    "#   - Hyperparameter tuning (GridSearchCV)\n",
    "#   - Export pipeline with joblib\n",
    "# ==============================================================\n",
    "\n",
    "# 1Ô∏è‚É£ Install Required Libraries (if not already installed)\n",
    "# Uncomment and run these lines once if needed\n",
    "# !pip install scikit-learn pandas joblib\n",
    "\n",
    "# 2Ô∏è‚É£ Import Necessary Libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# 3Ô∏è‚É£ Load Dataset (Telco Churn Dataset)\n",
    "# üëâ Make sure you have 'Telco-Customer-Churn.csv' in your working directory\n",
    "data = pd.read_csv(\"Telco-Customer-Churn.csv\")\n",
    "\n",
    "# 4Ô∏è‚É£ Basic Data Inspection\n",
    "print(\"üîπ Dataset Shape:\", data.shape)\n",
    "print(\"üîπ Columns:\", data.columns.tolist())\n",
    "print(data.head())\n",
    "\n",
    "# 5Ô∏è‚É£ Handle Missing Values (if any)\n",
    "data = data.dropna()\n",
    "\n",
    "# 6Ô∏è‚É£ Separate Features (X) and Target (y)\n",
    "X = data.drop(\"Churn\", axis=1)  # Features\n",
    "y = data[\"Churn\"]              # Target\n",
    "\n",
    "# 7Ô∏è‚É£ Identify Categorical & Numerical Columns\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "numerical_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "\n",
    "print(\"üîπ Categorical Columns:\", categorical_cols.tolist())\n",
    "print(\"üîπ Numerical Columns:\", numerical_cols.tolist())\n",
    "\n",
    "# 8Ô∏è‚É£ Preprocessing Pipelines\n",
    "# - OneHotEncoder for categorical\n",
    "# - StandardScaler for numerical\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numerical_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 9Ô∏è‚É£ Create a Machine Learning Pipeline\n",
    "# Here we try Logistic Regression and Random Forest\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# üîü Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£ Define Models & Hyperparameters for GridSearchCV\n",
    "param_grid = [\n",
    "    {\n",
    "        \"classifier\": [LogisticRegression(max_iter=1000)],\n",
    "        \"classifier__C\": [0.1, 1.0, 10.0]\n",
    "    },\n",
    "    {\n",
    "        \"classifier\": [RandomForestClassifier(random_state=42)],\n",
    "        \"classifier__n_estimators\": [50, 100],\n",
    "        \"classifier__max_depth\": [5, 10, None]\n",
    "    }\n",
    "]\n",
    "\n",
    "# 1Ô∏è‚É£2Ô∏è‚É£ Apply GridSearchCV for Best Model Selection\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline, param_grid, cv=3, n_jobs=-1, scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "print(\"‚è≥ Training and tuning the models...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 1Ô∏è‚É£3Ô∏è‚É£ Best Model from GridSearch\n",
    "print(\"‚úÖ Best Parameters:\", grid_search.best_params_)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# 1Ô∏è‚É£4Ô∏è‚É£ Model Evaluation\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"\\nüîπ Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"üîπ F1 Score:\", f1_score(y_test, y_pred, pos_label=\"Yes\"))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# 1Ô∏è‚É£5Ô∏è‚É£ Export the Trained Pipeline\n",
    "joblib.dump(best_model, \"churn_pipeline.pkl\")\n",
    "print(\"üì¶ Model pipeline saved as churn_pipeline.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
